{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача - найти идентичные товары по описанию; решим ее посредством обучения эмбеддингов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Библиотеки и установки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade nltk gensim bokeh\n",
    "#!pip install pytorch-metric-learning\n",
    "#!pip install record-keeper\n",
    "#!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:VERSION 1.3.0\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import csv\n",
    "import time\n",
    "import logging\n",
    "import record_keeper\n",
    "import unicodedata\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from cycler import cycler\n",
    "\n",
    "import pytorch_metric_learning\n",
    "import pytorch_metric_learning.utils.logging_presets as logging_presets\n",
    "from pytorch_metric_learning import losses, miners, samplers, testers, trainers\n",
    "from pytorch_metric_learning.utils import common_functions\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "from pytorch_metric_learning.distances import SNRDistance\n",
    "from pytorch_metric_learning.utils.inference import CustomKNN\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logging.info(\"VERSION %s\" % pytorch_metric_learning.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка, первичный анализ данных и постановка вопроса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_2278313361</td>\n",
       "      <td>PAPER BAG VICTORIA SECRET</td>\n",
       "      <td>249114794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
       "      <td>2395904891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3803689425</td>\n",
       "      <td>Maling Ham Pork Luncheon Meat TTS 397gr</td>\n",
       "      <td>2395904891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n",
       "      <td>4093212188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                              title  \\\n",
       "0   train_129225211                          Paper Bag Victoria Secret   \n",
       "1  train_2278313361                          PAPER BAG VICTORIA SECRET   \n",
       "2  train_2288590299        Maling TTS Canned Pork Luncheon Meat 397 gr   \n",
       "3  train_3803689425            Maling Ham Pork Luncheon Meat TTS 397gr   \n",
       "4  train_2406599165  Daster Batik Lengan pendek - Motif Acak / Camp...   \n",
       "\n",
       "   label_group  \n",
       "0    249114794  \n",
       "1    249114794  \n",
       "2   2395904891  \n",
       "3   2395904891  \n",
       "4   4093212188  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уникальным идентификатором товара является posting_id. Важно отметить, что все posting_id разные; мы будем действовать исходя из этого предположения касательно всего распределения - потенциальные повторы posting_id на инференсе (если они случатся) должны удаляться в контексте препроцессинга. <br>\n",
    "<br>\n",
    "Имеет смысл заострить внимание, что дубликаты в колонке title удалять нельзя, поскольку эти объекты (как и все остальные) имеют разный posting_id - в противном случае мы на ровном месте занижаем себе метрику, убирая самые легкие для модели случаи. <br>\n",
    "<br>\n",
    "Отметим также, что в среднем одному классу принадлежит 3-4 объекта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20952 20952 6608\n"
     ]
    }
   ],
   "source": [
    "print(df.shape[0], df['posting_id'].unique().shape[0], \n",
    "      df.label_group.unique().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n",
    "df['matches'] = df['label_group'].map(tmp)\n",
    "df['matches'] = df['matches'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "нашей метрикой будет построчный f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)]) \n",
    "    len_y_pred = y_pred.apply(lambda x: len(x)).values\n",
    "    len_y_true = y_true.apply(lambda x: len(x)).values\n",
    "    f1 = 2 * intersection / (len_y_pred + len_y_true)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "в качестве бейзлайна сопоставим себя в группу только с самим собой и каким нибудь еще одним случайным товаром"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prediction'] = df['posting_id'] + ' ' + 'train_129225211'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35675992543848534"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(df['matches'], df['prediction']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "цель улучшить эту метрику, пользоваться любыми методами\n",
    "\n",
    "на проде данных много и методы должны адаптироваться на большие данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Препроцессинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подобное переобозначение понадобится в дальнейшем, поскольку будет использоваться дизъюнктивный даталоадер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#хэш таблица с использованием label encoding\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "original_labels = list(set(df['label_group']))\n",
    "le.fit(original_labels)\n",
    "new_labels = le.transform(original_labels)\n",
    "hash_table = dict(zip(original_labels, new_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "      <th>matches</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>389</td>\n",
       "      <td>train_129225211 train_2278313361</td>\n",
       "      <td>train_129225211 train_129225211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_2278313361</td>\n",
       "      <td>PAPER BAG VICTORIA SECRET</td>\n",
       "      <td>389</td>\n",
       "      <td>train_129225211 train_2278313361</td>\n",
       "      <td>train_2278313361 train_129225211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
       "      <td>3672</td>\n",
       "      <td>train_2288590299 train_3803689425</td>\n",
       "      <td>train_2288590299 train_129225211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3803689425</td>\n",
       "      <td>Maling Ham Pork Luncheon Meat TTS 397gr</td>\n",
       "      <td>3672</td>\n",
       "      <td>train_2288590299 train_3803689425</td>\n",
       "      <td>train_3803689425 train_129225211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n",
       "      <td>6300</td>\n",
       "      <td>train_2406599165 train_3342059966</td>\n",
       "      <td>train_2406599165 train_129225211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20947</th>\n",
       "      <td>train_1001474240</td>\n",
       "      <td>Sam A20 A30 A30S A50 A50S A51 A71 A70 A70s M10...</td>\n",
       "      <td>5552</td>\n",
       "      <td>train_2944131255 train_1001474240</td>\n",
       "      <td>train_1001474240 train_129225211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20948</th>\n",
       "      <td>train_2244662893</td>\n",
       "      <td>LAMPU HURUF A-Z DAN ANGKA 0-9 \\xe2\\x9d\\xa4\\xef...</td>\n",
       "      <td>1148</td>\n",
       "      <td>train_2244662893 train_3281898016</td>\n",
       "      <td>train_2244662893 train_129225211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20949</th>\n",
       "      <td>train_3281898016</td>\n",
       "      <td>LAMPU HURUF A-Z DAN ANGKA 0-9 TINGGI 16 CM</td>\n",
       "      <td>1148</td>\n",
       "      <td>train_2244662893 train_3281898016</td>\n",
       "      <td>train_3281898016 train_129225211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20950</th>\n",
       "      <td>train_4221982820</td>\n",
       "      <td>Sprei Lady Rose 180x200 King terlaris Keroppi</td>\n",
       "      <td>75</td>\n",
       "      <td>train_4221982820 train_4063409014</td>\n",
       "      <td>train_4221982820 train_129225211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20951</th>\n",
       "      <td>train_4063409014</td>\n",
       "      <td>Sprei king ladyrose size 180x200 kerokeroppi</td>\n",
       "      <td>75</td>\n",
       "      <td>train_4221982820 train_4063409014</td>\n",
       "      <td>train_4063409014 train_129225211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20952 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             posting_id                                              title  \\\n",
       "0       train_129225211                          Paper Bag Victoria Secret   \n",
       "1      train_2278313361                          PAPER BAG VICTORIA SECRET   \n",
       "2      train_2288590299        Maling TTS Canned Pork Luncheon Meat 397 gr   \n",
       "3      train_3803689425            Maling Ham Pork Luncheon Meat TTS 397gr   \n",
       "4      train_2406599165  Daster Batik Lengan pendek - Motif Acak / Camp...   \n",
       "...                 ...                                                ...   \n",
       "20947  train_1001474240  Sam A20 A30 A30S A50 A50S A51 A71 A70 A70s M10...   \n",
       "20948  train_2244662893  LAMPU HURUF A-Z DAN ANGKA 0-9 \\xe2\\x9d\\xa4\\xef...   \n",
       "20949  train_3281898016         LAMPU HURUF A-Z DAN ANGKA 0-9 TINGGI 16 CM   \n",
       "20950  train_4221982820      Sprei Lady Rose 180x200 King terlaris Keroppi   \n",
       "20951  train_4063409014       Sprei king ladyrose size 180x200 kerokeroppi   \n",
       "\n",
       "       label_group                            matches  \\\n",
       "0              389   train_129225211 train_2278313361   \n",
       "1              389   train_129225211 train_2278313361   \n",
       "2             3672  train_2288590299 train_3803689425   \n",
       "3             3672  train_2288590299 train_3803689425   \n",
       "4             6300  train_2406599165 train_3342059966   \n",
       "...            ...                                ...   \n",
       "20947         5552  train_2944131255 train_1001474240   \n",
       "20948         1148  train_2244662893 train_3281898016   \n",
       "20949         1148  train_2244662893 train_3281898016   \n",
       "20950           75  train_4221982820 train_4063409014   \n",
       "20951           75  train_4221982820 train_4063409014   \n",
       "\n",
       "                             prediction  \n",
       "0       train_129225211 train_129225211  \n",
       "1      train_2278313361 train_129225211  \n",
       "2      train_2288590299 train_129225211  \n",
       "3      train_3803689425 train_129225211  \n",
       "4      train_2406599165 train_129225211  \n",
       "...                                 ...  \n",
       "20947  train_1001474240 train_129225211  \n",
       "20948  train_2244662893 train_129225211  \n",
       "20949  train_3281898016 train_129225211  \n",
       "20950  train_4221982820 train_129225211  \n",
       "20951  train_4063409014 train_129225211  \n",
       "\n",
       "[20952 rows x 5 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label_group'] = df['label_group'].apply(lambda x: hash_table[x])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Словарь символов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно собрать словарь всех символов, которые могут встречаться в title; необходимо сделать это в начале, до основных преобразований соответствующего столбца. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 69\n",
      "Characters:  !\"#$%&'()*+,-./0123456789:;<=>?@[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\n"
     ]
    }
   ],
   "source": [
    "all_chars = sorted(list(set(''.join(list(df['title'])).lower())))\n",
    "n_chars = len(all_chars)\n",
    "print('Total number of characters:', n_chars)\n",
    "print('Characters: ', *all_chars, sep='')\n",
    "\n",
    "#служебные словари\n",
    "char2idx = {key: value for value, key in enumerate(all_chars)}\n",
    "idx2char = {key: value for key, value in enumerate(all_chars)}\n",
    "\n",
    "def chars2indices(chars):\n",
    "    indices = []\n",
    "    for char in chars:\n",
    "        idx = char2idx[char]\n",
    "        indices.append(idx)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стандартный этап предобработки текстовых данных; использован токенизатор ntlk. Потенциально можно рассмотреть другие варианты токенизаций (напр., по n-грамам), и токенизаторов (например, YouTokenToMe от VK, в котором присутствует BPE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "      <th>matches</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>[paper, bag, victoria, secret]</td>\n",
       "      <td>389</td>\n",
       "      <td>train_129225211 train_2278313361</td>\n",
       "      <td>train_129225211 train_129225211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_2278313361</td>\n",
       "      <td>[paper, bag, victoria, secret]</td>\n",
       "      <td>389</td>\n",
       "      <td>train_129225211 train_2278313361</td>\n",
       "      <td>train_2278313361 train_129225211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>[maling, tts, canned, pork, luncheon, meat, 39...</td>\n",
       "      <td>3672</td>\n",
       "      <td>train_2288590299 train_3803689425</td>\n",
       "      <td>train_2288590299 train_129225211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3803689425</td>\n",
       "      <td>[maling, ham, pork, luncheon, meat, tts, 397gr]</td>\n",
       "      <td>3672</td>\n",
       "      <td>train_2288590299 train_3803689425</td>\n",
       "      <td>train_3803689425 train_129225211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>[daster, batik, lengan, pendek, -, motif, acak...</td>\n",
       "      <td>6300</td>\n",
       "      <td>train_2406599165 train_3342059966</td>\n",
       "      <td>train_2406599165 train_129225211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20947</th>\n",
       "      <td>train_1001474240</td>\n",
       "      <td>[sam, a20, a30, a30s, a50, a50s, a51, a71, a70...</td>\n",
       "      <td>5552</td>\n",
       "      <td>train_2944131255 train_1001474240</td>\n",
       "      <td>train_1001474240 train_129225211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20948</th>\n",
       "      <td>train_2244662893</td>\n",
       "      <td>[lampu, huruf, a, -, z, dan, angka, 0, -, 9, \\...</td>\n",
       "      <td>1148</td>\n",
       "      <td>train_2244662893 train_3281898016</td>\n",
       "      <td>train_2244662893 train_129225211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20949</th>\n",
       "      <td>train_3281898016</td>\n",
       "      <td>[lampu, huruf, a, -, z, dan, angka, 0, -, 9, t...</td>\n",
       "      <td>1148</td>\n",
       "      <td>train_2244662893 train_3281898016</td>\n",
       "      <td>train_3281898016 train_129225211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20950</th>\n",
       "      <td>train_4221982820</td>\n",
       "      <td>[sprei, lady, rose, 180x200, king, terlaris, k...</td>\n",
       "      <td>75</td>\n",
       "      <td>train_4221982820 train_4063409014</td>\n",
       "      <td>train_4221982820 train_129225211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20951</th>\n",
       "      <td>train_4063409014</td>\n",
       "      <td>[sprei, king, ladyrose, size, 180x200, keroker...</td>\n",
       "      <td>75</td>\n",
       "      <td>train_4221982820 train_4063409014</td>\n",
       "      <td>train_4063409014 train_129225211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20952 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             posting_id                                              title  \\\n",
       "0       train_129225211                     [paper, bag, victoria, secret]   \n",
       "1      train_2278313361                     [paper, bag, victoria, secret]   \n",
       "2      train_2288590299  [maling, tts, canned, pork, luncheon, meat, 39...   \n",
       "3      train_3803689425    [maling, ham, pork, luncheon, meat, tts, 397gr]   \n",
       "4      train_2406599165  [daster, batik, lengan, pendek, -, motif, acak...   \n",
       "...                 ...                                                ...   \n",
       "20947  train_1001474240  [sam, a20, a30, a30s, a50, a50s, a51, a71, a70...   \n",
       "20948  train_2244662893  [lampu, huruf, a, -, z, dan, angka, 0, -, 9, \\...   \n",
       "20949  train_3281898016  [lampu, huruf, a, -, z, dan, angka, 0, -, 9, t...   \n",
       "20950  train_4221982820  [sprei, lady, rose, 180x200, king, terlaris, k...   \n",
       "20951  train_4063409014  [sprei, king, ladyrose, size, 180x200, keroker...   \n",
       "\n",
       "       label_group                            matches  \\\n",
       "0              389   train_129225211 train_2278313361   \n",
       "1              389   train_129225211 train_2278313361   \n",
       "2             3672  train_2288590299 train_3803689425   \n",
       "3             3672  train_2288590299 train_3803689425   \n",
       "4             6300  train_2406599165 train_3342059966   \n",
       "...            ...                                ...   \n",
       "20947         5552  train_2944131255 train_1001474240   \n",
       "20948         1148  train_2244662893 train_3281898016   \n",
       "20949         1148  train_2244662893 train_3281898016   \n",
       "20950           75  train_4221982820 train_4063409014   \n",
       "20951           75  train_4221982820 train_4063409014   \n",
       "\n",
       "                             prediction  \n",
       "0       train_129225211 train_129225211  \n",
       "1      train_2278313361 train_129225211  \n",
       "2      train_2288590299 train_129225211  \n",
       "3      train_3803689425 train_129225211  \n",
       "4      train_2406599165 train_129225211  \n",
       "...                                 ...  \n",
       "20947  train_1001474240 train_129225211  \n",
       "20948  train_2244662893 train_129225211  \n",
       "20949  train_3281898016 train_129225211  \n",
       "20950  train_4221982820 train_129225211  \n",
       "20951  train_4063409014 train_129225211  \n",
       "\n",
       "[20952 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "df['title'] = df['title'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка предобученных эмбеддингов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве предобученных эмбеддингов были выбраны twitter-25 из библиотеки gensim. Легкие, менее академичные, чем wiki и, если не ошибаюсь, был задействован меньший window size, что является плюсом с учетом меньшей связности подобных текстов. Возможно, очень интересным вариантом могло бы быть использование специальных эмбеддингов для ecommerce, например, как описано в статье \"Query2Prod2Vec:\n",
    "Grounded Word Embeddings for eCommerce\" (https://arxiv.org/abs/2104.02061). С другой стороны, большой плюс эмбеддингов twitter в многоязычности, где все языки находятся в одном семантическом пространстве. Поскольку мы имеем дело с данными индонезийского ecommerce (смешанный индонезийско-английский текст), это очень весомое преимущество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.keyedvectors:loading projection weights from /Users/andreyegorov/gensim-data/glove-twitter-25/glove-twitter-25.gz\n",
      "INFO:gensim.utils:KeyedVectors lifecycle event {'msg': 'loaded (1193514, 25) matrix of type float32 from /Users/andreyegorov/gensim-data/glove-twitter-25/glove-twitter-25.gz', 'binary': False, 'encoding': 'utf8', 'datetime': '2022-04-17T18:46:43.059618', 'gensim': '4.1.2', 'python': '3.8.11 (default, Aug  6 2021, 08:56:27) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "glove_vectors = gensim.downloader.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jepang\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('inggris', 0.8793800473213196),\n",
       " ('eropa', 0.8693690896034241),\n",
       " ('indonesia', 0.8535388708114624),\n",
       " ('jawa', 0.8527381420135498),\n",
       " ('budaya', 0.8480342626571655),\n",
       " ('belanda', 0.844948947429657),\n",
       " ('perancis', 0.8439698815345764),\n",
       " ('versi', 0.8357038497924805),\n",
       " ('jerman', 0.8328619003295898),\n",
       " ('utama', 0.8298693895339966)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#семантическое пространство векторов покрывает индонезийский язык\n",
    "print(df['title'][5][3])\n",
    "glove_vectors.most_similar(df['title'][5][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Каскадный принцип разделения на векторизуемые слова и символы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В связи со спецификой текста, кажется неверным полностью избавляться от невекторизируемых слов (для которых отсутствует предобученный эмбеддинг), заменяя их, допустим, на токен UNK. В таких словах может содержаться важная информация о технических характеристиках товара, указывающая на сходство (например, '10ml'). На мой взгляд, логично организовать векторизацию по каскадному принципу - отделить невекторизуемые слова, и векторизовать их уже на уровне символов, соответственно, в совокупности с обучаемыми с нуля эмбеддингами. Каждая из двух частей будет обрабатываться своим рекуррентным модулем в составе общей сети. По сути, мы отказываемся от токена UNK.  Кажется также, что не векторизуемые слова перед посимвольной векторизацией лучше отсортировать, чтобы зафиксировать некое примерное отношение порядка, скажем, тех же технических характеристик."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_vectorized(array):\n",
    "    '''Разбиваем массив на векторизуемые данными эмбеддингами слова, и все остальные.\n",
    "    Не векторизуемые упорядочиваем'''\n",
    "    master_array = []\n",
    "    word_level = []\n",
    "    char_level = []\n",
    "    for item in array:\n",
    "        try:\n",
    "            word_level.append(glove_vectors.key_to_index[item])\n",
    "        except:\n",
    "            char_level.append(item)\n",
    "    master_array.append(word_level)\n",
    "    master_array.append(sorted(char_level))\n",
    "    \n",
    "    return master_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>word_level</th>\n",
       "      <th>char_level</th>\n",
       "      <th>label_group</th>\n",
       "      <th>matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>[2263, 2417, 3170, 2073]</td>\n",
       "      <td>[]</td>\n",
       "      <td>389</td>\n",
       "      <td>train_129225211 train_2278313361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_2278313361</td>\n",
       "      <td>[2263, 2417, 3170, 2073]</td>\n",
       "      <td>[]</td>\n",
       "      <td>389</td>\n",
       "      <td>train_129225211 train_2278313361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>[23905, 22807, 62077, 12894, 80866, 7050, 3413]</td>\n",
       "      <td>[397]</td>\n",
       "      <td>3672</td>\n",
       "      <td>train_2288590299 train_3803689425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3803689425</td>\n",
       "      <td>[23905, 5442, 12894, 80866, 7050, 22807]</td>\n",
       "      <td>[397gr]</td>\n",
       "      <td>3672</td>\n",
       "      <td>train_2288590299 train_3803689425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>[131370, 18071, 46352, 11005, 28, 32674, 37431...</td>\n",
       "      <td>[00, alhadi, dpt001]</td>\n",
       "      <td>6300</td>\n",
       "      <td>train_2406599165 train_3342059966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20947</th>\n",
       "      <td>train_1001474240</td>\n",
       "      <td>[2630, 28, 105361, 4088, 369, 234, 986]</td>\n",
       "      <td>[a20, a30, a30s, a31, a50, a50s, a51, a70, a70...</td>\n",
       "      <td>5552</td>\n",
       "      <td>train_2944131255 train_1001474240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20948</th>\n",
       "      <td>train_2244662893</td>\n",
       "      <td>[8227, 14402, 11, 28, 1016, 233, 18988, 28, 37...</td>\n",
       "      <td>[0, 0, 16, 9, 9, x8f, x9d, xa4, xb8, xe2, xef]</td>\n",
       "      <td>1148</td>\n",
       "      <td>train_2244662893 train_3281898016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20949</th>\n",
       "      <td>train_3281898016</td>\n",
       "      <td>[8227, 14402, 11, 28, 1016, 233, 18988, 28, 44...</td>\n",
       "      <td>[0, 16, 9]</td>\n",
       "      <td>1148</td>\n",
       "      <td>train_2244662893 train_3281898016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20950</th>\n",
       "      <td>train_4221982820</td>\n",
       "      <td>[161903, 1404, 3902, 1696, 162980, 426544]</td>\n",
       "      <td>[180x200]</td>\n",
       "      <td>75</td>\n",
       "      <td>train_4221982820 train_4063409014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20951</th>\n",
       "      <td>train_4063409014</td>\n",
       "      <td>[161903, 1696, 1014246, 2361]</td>\n",
       "      <td>[180x200, kerokeroppi]</td>\n",
       "      <td>75</td>\n",
       "      <td>train_4221982820 train_4063409014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20952 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             posting_id                                         word_level  \\\n",
       "0       train_129225211                           [2263, 2417, 3170, 2073]   \n",
       "1      train_2278313361                           [2263, 2417, 3170, 2073]   \n",
       "2      train_2288590299    [23905, 22807, 62077, 12894, 80866, 7050, 3413]   \n",
       "3      train_3803689425           [23905, 5442, 12894, 80866, 7050, 22807]   \n",
       "4      train_2406599165  [131370, 18071, 46352, 11005, 28, 32674, 37431...   \n",
       "...                 ...                                                ...   \n",
       "20947  train_1001474240            [2630, 28, 105361, 4088, 369, 234, 986]   \n",
       "20948  train_2244662893  [8227, 14402, 11, 28, 1016, 233, 18988, 28, 37...   \n",
       "20949  train_3281898016  [8227, 14402, 11, 28, 1016, 233, 18988, 28, 44...   \n",
       "20950  train_4221982820         [161903, 1404, 3902, 1696, 162980, 426544]   \n",
       "20951  train_4063409014                      [161903, 1696, 1014246, 2361]   \n",
       "\n",
       "                                              char_level  label_group  \\\n",
       "0                                                     []          389   \n",
       "1                                                     []          389   \n",
       "2                                                  [397]         3672   \n",
       "3                                                [397gr]         3672   \n",
       "4                                   [00, alhadi, dpt001]         6300   \n",
       "...                                                  ...          ...   \n",
       "20947  [a20, a30, a30s, a31, a50, a50s, a51, a70, a70...         5552   \n",
       "20948     [0, 0, 16, 9, 9, x8f, x9d, xa4, xb8, xe2, xef]         1148   \n",
       "20949                                         [0, 16, 9]         1148   \n",
       "20950                                          [180x200]           75   \n",
       "20951                             [180x200, kerokeroppi]           75   \n",
       "\n",
       "                                 matches  \n",
       "0       train_129225211 train_2278313361  \n",
       "1       train_129225211 train_2278313361  \n",
       "2      train_2288590299 train_3803689425  \n",
       "3      train_2288590299 train_3803689425  \n",
       "4      train_2406599165 train_3342059966  \n",
       "...                                  ...  \n",
       "20947  train_2944131255 train_1001474240  \n",
       "20948  train_2244662893 train_3281898016  \n",
       "20949  train_2244662893 train_3281898016  \n",
       "20950  train_4221982820 train_4063409014  \n",
       "20951  train_4221982820 train_4063409014  \n",
       "\n",
       "[20952 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#разбиваем на часть, которая векторизуется по словам, и ту, которая нет\n",
    "df['title'] = df['title'].apply(split_vectorized)\n",
    "df_temp = pd.DataFrame(df[\"title\"].to_list(), columns=['word_level', 'char_level'])\n",
    "df = pd.concat([df[['posting_id']], df_temp, df[['label_group', 'matches']]], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>word_level</th>\n",
       "      <th>char_level</th>\n",
       "      <th>label_group</th>\n",
       "      <th>matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>[2263, 2417, 3170, 2073]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>389</td>\n",
       "      <td>train_129225211 train_2278313361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_2278313361</td>\n",
       "      <td>[2263, 2417, 3170, 2073]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>389</td>\n",
       "      <td>train_129225211 train_2278313361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>[23905, 22807, 62077, 12894, 80866, 7050, 3413]</td>\n",
       "      <td>[19, 25, 23]</td>\n",
       "      <td>3672</td>\n",
       "      <td>train_2288590299 train_3803689425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3803689425</td>\n",
       "      <td>[23905, 5442, 12894, 80866, 7050, 22807]</td>\n",
       "      <td>[19, 25, 23, 45, 56]</td>\n",
       "      <td>3672</td>\n",
       "      <td>train_2288590299 train_3803689425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>[131370, 18071, 46352, 11005, 28, 32674, 37431...</td>\n",
       "      <td>[16, 16, 0, 39, 50, 46, 39, 42, 47, 0, 42, 54,...</td>\n",
       "      <td>6300</td>\n",
       "      <td>train_2406599165 train_3342059966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20947</th>\n",
       "      <td>train_1001474240</td>\n",
       "      <td>[2630, 28, 105361, 4088, 369, 234, 986]</td>\n",
       "      <td>[39, 18, 16, 0, 39, 19, 16, 0, 39, 19, 16, 57,...</td>\n",
       "      <td>5552</td>\n",
       "      <td>train_2944131255 train_1001474240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20948</th>\n",
       "      <td>train_2244662893</td>\n",
       "      <td>[8227, 14402, 11, 28, 1016, 233, 18988, 28, 37...</td>\n",
       "      <td>[16, 0, 16, 0, 17, 22, 0, 25, 0, 25, 0, 62, 24...</td>\n",
       "      <td>1148</td>\n",
       "      <td>train_2244662893 train_3281898016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20949</th>\n",
       "      <td>train_3281898016</td>\n",
       "      <td>[8227, 14402, 11, 28, 1016, 233, 18988, 28, 44...</td>\n",
       "      <td>[16, 0, 17, 22, 0, 25]</td>\n",
       "      <td>1148</td>\n",
       "      <td>train_2244662893 train_3281898016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20950</th>\n",
       "      <td>train_4221982820</td>\n",
       "      <td>[161903, 1404, 3902, 1696, 162980, 426544]</td>\n",
       "      <td>[17, 24, 16, 62, 18, 16, 16]</td>\n",
       "      <td>75</td>\n",
       "      <td>train_4221982820 train_4063409014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20951</th>\n",
       "      <td>train_4063409014</td>\n",
       "      <td>[161903, 1696, 1014246, 2361]</td>\n",
       "      <td>[17, 24, 16, 62, 18, 16, 16, 0, 49, 43, 56, 53...</td>\n",
       "      <td>75</td>\n",
       "      <td>train_4221982820 train_4063409014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20952 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             posting_id                                         word_level  \\\n",
       "0       train_129225211                           [2263, 2417, 3170, 2073]   \n",
       "1      train_2278313361                           [2263, 2417, 3170, 2073]   \n",
       "2      train_2288590299    [23905, 22807, 62077, 12894, 80866, 7050, 3413]   \n",
       "3      train_3803689425           [23905, 5442, 12894, 80866, 7050, 22807]   \n",
       "4      train_2406599165  [131370, 18071, 46352, 11005, 28, 32674, 37431...   \n",
       "...                 ...                                                ...   \n",
       "20947  train_1001474240            [2630, 28, 105361, 4088, 369, 234, 986]   \n",
       "20948  train_2244662893  [8227, 14402, 11, 28, 1016, 233, 18988, 28, 37...   \n",
       "20949  train_3281898016  [8227, 14402, 11, 28, 1016, 233, 18988, 28, 44...   \n",
       "20950  train_4221982820         [161903, 1404, 3902, 1696, 162980, 426544]   \n",
       "20951  train_4063409014                      [161903, 1696, 1014246, 2361]   \n",
       "\n",
       "                                              char_level  label_group  \\\n",
       "0                                                    [0]          389   \n",
       "1                                                    [0]          389   \n",
       "2                                           [19, 25, 23]         3672   \n",
       "3                                   [19, 25, 23, 45, 56]         3672   \n",
       "4      [16, 16, 0, 39, 50, 46, 39, 42, 47, 0, 42, 54,...         6300   \n",
       "...                                                  ...          ...   \n",
       "20947  [39, 18, 16, 0, 39, 19, 16, 0, 39, 19, 16, 57,...         5552   \n",
       "20948  [16, 0, 16, 0, 17, 22, 0, 25, 0, 25, 0, 62, 24...         1148   \n",
       "20949                             [16, 0, 17, 22, 0, 25]         1148   \n",
       "20950                       [17, 24, 16, 62, 18, 16, 16]           75   \n",
       "20951  [17, 24, 16, 62, 18, 16, 16, 0, 49, 43, 56, 53...           75   \n",
       "\n",
       "                                 matches  \n",
       "0       train_129225211 train_2278313361  \n",
       "1       train_129225211 train_2278313361  \n",
       "2      train_2288590299 train_3803689425  \n",
       "3      train_2288590299 train_3803689425  \n",
       "4      train_2406599165 train_3342059966  \n",
       "...                                  ...  \n",
       "20947  train_2944131255 train_1001474240  \n",
       "20948  train_2244662893 train_3281898016  \n",
       "20949  train_2244662893 train_3281898016  \n",
       "20950  train_4221982820 train_4063409014  \n",
       "20951  train_4221982820 train_4063409014  \n",
       "\n",
       "[20952 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#преобразуем char level\n",
    "df['char_level'] = df['char_level'].apply(lambda x: ' '.join(x))\n",
    "df['char_level'] = df['char_level'].apply(chars2indices)\n",
    "#заплатка с empty list\n",
    "df['char_level'] = df['char_level'].apply(lambda x: [0] if len(x) == 0 else x)\n",
    "df['word_level'] = df['word_level'].apply(lambda x: [0] if len(x) == 0 else x)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Начало сборки в датасеты / даталоадеры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В начале прописан набор служебных упаковщиков/распаковщиков тензоров с кодированием разбиения; написал их, чтобы оптимальным образом состыковаться с интерфейсом классов библиотеки PyTorch Metric Learning (дальше PML). <br>\n",
    "<br>\n",
    "Даталоадер - на основе ноутбука: <br>\n",
    "https://github.com/KevinMusgrave/pytorch-metric-learning/blob/master/examples/notebooks/MetricLossOnly.ipynb <br>\n",
    "Его особенность в том, что он дизъюнктивный, то есть на тесте модель не видит тех классов, которые видела на трейне - это необходимо для специфики задач metric learning, чтобы подтолкнуть модель фокусироваться на выделении отношений между классами объектов, в противовес характеристикам конкретного класса (или классов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack(tensor1, tensor2):\n",
    "    '''input: two non-empty 1d tensors\n",
    "    output: 1d tensor with splitting pointer at index [0]'''\n",
    "    pointer = torch.tensor([tensor1.shape[0]])\n",
    "    return torch.cat([pointer, tensor1, tensor2])\n",
    "\n",
    "def unpack(packed):\n",
    "    '''input: 1d tensor with splitting pointer at index [0]\n",
    "    output: two 1d tensors split at pointer value'''\n",
    "    pointer = int(packed[0])\n",
    "    data = packed[1:]\n",
    "    return data[:pointer], data[pointer:]\n",
    "\n",
    "def batch_pack(tensor1, tensor2):\n",
    "    '''input: two non-empty 2d tensors with batch size at dim 0\n",
    "    output: 2d tensor with splitting pointer'''\n",
    "    assert tensor1.shape[0] == tensor2.shape[0]\n",
    "    bs = tensor1.shape[0]\n",
    "    pointer = torch.tensor([tensor1.shape[1]]).expand(bs, -1)\n",
    "    return torch.cat([pointer, tensor1, tensor2], dim=1)\n",
    "\n",
    "def batch_unpack(packed):\n",
    "    '''input: 2d tensor with splitting pointer\n",
    "    output: two non-empty 2d tensors split at pointer value'''\n",
    "    pointer = torch.unique(packed[:, 0])\n",
    "    assert pointer.shape[0] == 1\n",
    "    pointer = int(pointer[0])\n",
    "    data = packed[:, 1:]\n",
    "    return data[:, :pointer], data[:, pointer:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntriesDisjointDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, train_flag):\n",
    "        threshold = int(np.percentile(df['label_group'], 80))\n",
    "        rule = (lambda x: x < threshold) if train_flag else (lambda x: x >= threshold)\n",
    "        filtered_idx = [int(i) for i, x in enumerate(df['label_group']) if rule(x)]\n",
    "        self.word_level = df['word_level'].iloc[filtered_idx].reset_index(drop=True)\n",
    "        self.char_level = df['char_level'].iloc[filtered_idx].reset_index(drop=True)\n",
    "        self.labels = df['label_group'].iloc[filtered_idx].reset_index(drop=True)\n",
    "        \n",
    "        if not train_flag:\n",
    "            global val_idx\n",
    "            val_idx = filtered_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "            x = pack(torch.tensor(self.word_level[idx]), \n",
    "                                  torch.tensor(self.char_level[idx]))\n",
    "            y = torch.tensor(self.labels[idx])\n",
    "            return (x, y)\n",
    "\n",
    "# Class disjoint training and validation set\n",
    "train_dataset = EntriesDisjointDataset(df, True)\n",
    "val_dataset = EntriesDisjointDataset(df, False)\n",
    "assert set(train_dataset.labels).isdisjoint(set(val_dataset.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    '''Сollate_fn для корректного формирования даталоадеров \n",
    "    внутри trainer/tester из библиотеки PML. Формат выхода функции\n",
    "    (x, y) синхронизирован с интерфейсом указанных классов'''\n",
    "    word_level_arr = []\n",
    "    char_level_arr = []\n",
    "    label_arr = []\n",
    "    for item in batch: \n",
    "        \n",
    "        x, y = item\n",
    "        word_level, char_level = unpack(x)\n",
    "        word_level_arr.append(word_level)\n",
    "        char_level_arr.append(char_level)\n",
    "        label_arr.append(y)\n",
    "\n",
    "    word_level_arr = pad_sequence(word_level_arr, batch_first=True)\n",
    "    char_level_arr = pad_sequence(char_level_arr, batch_first=True)\n",
    "    \n",
    "    x = batch_pack(word_level_arr, char_level_arr)\n",
    "    y = torch.tensor(label_arr)\n",
    "    \n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Конфигурация сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM - надежный базовый выбор для подобных задач; в частности, сведения последовательности переменной длины, где важен - хотя бы относительно - порядок, в вектор фиксированного размера. Блоки двунаправленные (поскольку сразу доступно все описание целиком), а также с двумя слоями для потенциального выделения более высокоуровневых признаков. Альтернативно можно было бы рассмотреть сверточную сеть с max pool over time (для выравнивания различий в длинах входящих последовательностей), а также трансформеры. Последние, как известно, очень удобно параллелизовать, что было бы преимуществом в контексте более широкой задачи.<br>\n",
    "<br>\n",
    "Из некоторых особенностей - мы замораживаем веса предобученных эмбеддингов, чтобы не произошло их дальнейшее переобучение вследствие небольшого размера выборки (в общем случае, для задач similarity рекомендации входные эмбеддинги дообучать). Для посимвольной ветви, разумеется, вариантов нет - используем обучаемый embedding слой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#получаем веса эмбеддингов\n",
    "embedding_weights = torch.FloatTensor(glove_vectors.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MetricLSTM, self).__init__()\n",
    "     \n",
    "        self.word_embedding = nn.Embedding.from_pretrained(embedding_weights, \n",
    "                                                      freeze=True,\n",
    "                                                      padding_idx=0)\n",
    "   \n",
    "        self.word_lstm = nn.LSTM(input_size=25,\n",
    "                            hidden_size=25,\n",
    "                            num_layers=2,\n",
    "                            batch_first=True, \n",
    "                            bidirectional=True)\n",
    "        \n",
    "        self.char_embedding = nn.Embedding(num_embeddings=n_chars, #было плохо\n",
    "                                           embedding_dim=25, \n",
    "                                           padding_idx=0)\n",
    "        \n",
    "        self.char_lstm = nn.LSTM(input_size=25,\n",
    "                                hidden_size=25,\n",
    "                                num_layers=2,\n",
    "                                batch_first=True, \n",
    "                                bidirectional=True)\n",
    "        \n",
    "        self.linear = nn.Linear(25*2*2*2, 100)\n",
    "    \n",
    "    def forward_branch(self, x, embedding, lstm):\n",
    "        x = embedding(x) #(N, L, 25)\n",
    "        x = lstm(x)[1][0] #(4, N, 25)\n",
    "        x = x.permute(1, 0, -1) #(N, 4, 25)\n",
    "        x = torch.cat((torch.chunk(x, 4, dim=1)[0], \n",
    "                       torch.chunk(x, 4, dim=1)[1],\n",
    "                       torch.chunk(x, 4, dim=1)[2],\n",
    "                       torch.chunk(x, 4, dim=1)[3]), dim=2)\n",
    "                        #(N, 1, 100)\n",
    "        x = x.squeeze(dim=1) #(N, 100)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_word, x_char = batch_unpack(x)\n",
    "        x_word = self.forward_branch(x_word, self.word_embedding,\n",
    "                                    self.word_lstm)\n",
    "        x_char = self.forward_branch(x_char, self.char_embedding,\n",
    "                                    self.char_lstm)\n",
    "        x = torch.cat([x_word, x_char], dim=1)\n",
    "        embedding = self.linear(x)\n",
    "        \n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Процесс обучения адаптирован по следующим ноутбукам: <br>\n",
    "https://github.com/KevinMusgrave/pytorch-metric-learning/blob/master/examples/notebooks/MetricLossOnly.ipynb\n",
    "https://github.com/KevinMusgrave/pytorch-metric-learning/blob/master/examples/notebooks/scRNAseq_MetricEmbedding.ipynb <br>\n",
    "<br>\n",
    "Нашей искомой построчной f1 метрики нет в стандартных метриках PML, поэтому обучение контролируем по сильно коррелирующей с ней прокси-метрике (mean average precision at r из рекомендованной статьи Metric Learning: Reality Check). В идеале, было бы хорошо реализовать необходимую нам метрику как custom средствами библиотеки; в этом случае, помимо прочего, непосредственно по ней можно будет делать early stopping (параметр patience в end of epoch hook) и т.д.<br>\n",
    "<br>\n",
    "В качестве лосс-функции был выбран triplet loss (в сочетании с определенными функции подбора объектов при обучении), но здесь остается достаточно широкое пространство для экспериментирования и потенциального улучшения. Особо интересно выглядят методы с \"искусственным центром\" (Center Loss, SphereFace, ArcFace, CosFace), применяющиеся в SOTA моделях; их реализация есть в PML. <br> \n",
    "<br>\n",
    "Некоторые затруднения вызвала некорректная локальная работа библиотеки faiss, на которую, в свою очередь, завязана часть PML; большинство затруднений удалось преодолеть через кастомизацию (например, исключив некоторые метрики из списка стандартно рассчитываемых, прописав отдельно функцию для KNN и тд), но в целом вопрос налаживания ее корректной работы очень важен."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация модели и параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "trunk = MetricLSTM()\n",
    "trunk_optimizer = torch.optim.Adam(trunk.parameters(), \n",
    "                                   lr=0.001,\n",
    "                                   weight_decay=0.0001)\n",
    "\n",
    "loss = losses.TripletMarginLoss(margin=0.1)\n",
    "\n",
    "#Функция майнинга триплетов (определяет стратегию майнинга)\n",
    "miner = miners.MultiSimilarityMiner(epsilon=0.1)\n",
    "\n",
    "#Сэмплер\n",
    "sampler = samplers.MPerClassSampler(train_dataset.labels, m=2, \n",
    "                                    length_before_new_iter=len(train_dataset))\n",
    "#m=2, так как у нас может и не быть более чем двух примеров одного класса в целом батче\n",
    "\n",
    "batch_size = 256\n",
    "num_epochs = 50\n",
    "\n",
    "#Запаковываем в словари\n",
    "models = {\"trunk\": trunk}\n",
    "optimizers = {\"trunk_optimizer\": trunk_optimizer}\n",
    "loss_funcs = {\"metric_loss\": loss}\n",
    "mining_funcs = {\"tuple_miner\": miner}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Спец процедуры для вызова в процессе обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#главный контейнер процедур\n",
    "record_keeper, _, _ = logging_presets.get_record_keeper(\"metriclstm_logs\", \n",
    "                                                        \"metriclstm_tensorboard\")\n",
    "\n",
    "hooks = logging_presets.get_hook_container(record_keeper,\n",
    "                                            record_group_name_prefix=None, \n",
    "                                            primary_metric=\"mean_average_precision_at_r\", \n",
    "                                            validation_split_name=\"val\",\n",
    "                                            save_models=True,\n",
    "                                            log_freq=50) \n",
    "\n",
    "#тестировщик\n",
    "knn_func = CustomKNN(SNRDistance())\n",
    "accuracy_calculator = AccuracyCalculator(exclude=(\"NMI\", \"AMI\"), \n",
    "                                        knn_func=knn_func)\n",
    "\n",
    "tester = testers.GlobalEmbeddingSpaceTester(\n",
    "                    end_of_testing_hook=hooks.end_of_testing_hook,\n",
    "                    batch_size=128,\n",
    "                    dataloader_num_workers=0,\n",
    "                    accuracy_calculator=accuracy_calculator)\n",
    "\n",
    "#процедура в конце эпохи\n",
    "dataset_dict = {\"val\": val_dataset} \n",
    "model_folder = \"metriclstm_saved_models\"\n",
    "\n",
    "end_of_epoch_hook = hooks.end_of_epoch_hook(tester, dataset_dict, model_folder, \n",
    "                                            test_interval=1, patience=5,\n",
    "                                            test_collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сборка и инициализация trainer'a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:PML:models is missing \"embedder\"\n"
     ]
    }
   ],
   "source": [
    "trainer = trainers.MetricLossOnly(\n",
    "    models,\n",
    "    optimizers,\n",
    "    batch_size,\n",
    "    loss_funcs,\n",
    "    mining_funcs,\n",
    "    train_dataset,\n",
    "    sampler=sampler,\n",
    "    collate_fn=collate_fn,\n",
    "    dataloader_num_workers=0,\n",
    "    end_of_iteration_hook=hooks.end_of_iteration_hook,\n",
    "    end_of_epoch_hook=end_of_epoch_hook,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запуск визуализации в Тензорборд"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ad0e0575edd9723c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ad0e0575edd9723c\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir metriclstm_tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель обучается в рабочем режиме; обучение заканчивается на 40-й эпохе, когда метрика выходит на плато при условии достаточно щадящего значения patience=5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:PML:Initializing dataloader\n",
      "INFO:PML:Initializing dataloader iterator\n",
      "INFO:PML:Done creating dataloader iterator\n",
      "INFO:PML:TRAINING EPOCH 1\n",
      "total_loss=0.11147: 100%|███████████████████████| 41/41 [00:15<00:00,  2.72it/s]\n",
      "INFO:PML:Evaluating epoch 1\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.22it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:New best accuracy! 0.3506856742770156\n",
      "INFO:PML:TRAINING EPOCH 2\n",
      "total_loss=0.12309: 100%|███████████████████████| 41/41 [00:15<00:00,  2.71it/s]\n",
      "INFO:PML:Evaluating epoch 2\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 14.71it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:New best accuracy! 0.3900538907389589\n",
      "INFO:PML:TRAINING EPOCH 3\n",
      "total_loss=0.10559: 100%|███████████████████████| 41/41 [00:16<00:00,  2.50it/s]\n",
      "INFO:PML:Evaluating epoch 3\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.08it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:New best accuracy! 0.4149965830037477\n",
      "INFO:PML:TRAINING EPOCH 4\n",
      "total_loss=0.09691: 100%|███████████████████████| 41/41 [00:14<00:00,  2.74it/s]\n",
      "INFO:PML:Evaluating epoch 4\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 14.97it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:New best accuracy! 0.4255225916521498\n",
      "INFO:PML:TRAINING EPOCH 5\n",
      "total_loss=0.09510: 100%|███████████████████████| 41/41 [00:15<00:00,  2.70it/s]\n",
      "INFO:PML:Evaluating epoch 5\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.06it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:New best accuracy! 0.44736522839098075\n",
      "INFO:PML:TRAINING EPOCH 6\n",
      "total_loss=0.08831: 100%|███████████████████████| 41/41 [00:14<00:00,  2.77it/s]\n",
      "INFO:PML:Evaluating epoch 6\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.08it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:New best accuracy! 0.45783546143923604\n",
      "INFO:PML:TRAINING EPOCH 7\n",
      "total_loss=0.08373: 100%|███████████████████████| 41/41 [00:15<00:00,  2.68it/s]\n",
      "INFO:PML:Evaluating epoch 7\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.09it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:New best accuracy! 0.4634244862877917\n",
      "INFO:PML:TRAINING EPOCH 8\n",
      "total_loss=0.07125: 100%|███████████████████████| 41/41 [00:15<00:00,  2.73it/s]\n",
      "INFO:PML:Evaluating epoch 8\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.08it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:New best accuracy! 0.46925776302755945\n",
      "INFO:PML:TRAINING EPOCH 9\n",
      "total_loss=0.10491: 100%|███████████████████████| 41/41 [00:15<00:00,  2.63it/s]\n",
      "INFO:PML:Evaluating epoch 9\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.11it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:New best accuracy! 0.48107701747101383\n",
      "INFO:PML:TRAINING EPOCH 10\n",
      "total_loss=0.09546: 100%|███████████████████████| 41/41 [00:14<00:00,  2.78it/s]\n",
      "INFO:PML:Evaluating epoch 10\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 14.99it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:New best accuracy! 0.4857419767312348\n",
      "INFO:PML:TRAINING EPOCH 11\n",
      "total_loss=0.07668: 100%|███████████████████████| 41/41 [00:14<00:00,  2.89it/s]\n",
      "INFO:PML:Evaluating epoch 11\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.14it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:New best accuracy! 0.490880117915107\n",
      "INFO:PML:TRAINING EPOCH 12\n",
      "total_loss=0.08132: 100%|███████████████████████| 41/41 [00:15<00:00,  2.71it/s]\n",
      "INFO:PML:Evaluating epoch 12\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.10it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:New best accuracy! 0.5006190281884004\n",
      "INFO:PML:TRAINING EPOCH 13\n",
      "total_loss=0.10858: 100%|███████████████████████| 41/41 [00:15<00:00,  2.63it/s]\n",
      "INFO:PML:Evaluating epoch 13\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 14.97it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:New best accuracy! 0.5035579822855988\n",
      "INFO:PML:TRAINING EPOCH 14\n",
      "total_loss=0.08796: 100%|███████████████████████| 41/41 [00:15<00:00,  2.68it/s]\n",
      "INFO:PML:Evaluating epoch 14\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.09it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:New best accuracy! 0.5076366433244119\n",
      "INFO:PML:TRAINING EPOCH 15\n",
      "total_loss=0.07756: 100%|███████████████████████| 41/41 [00:14<00:00,  2.77it/s]\n",
      "INFO:PML:Evaluating epoch 15\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.04it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:TRAINING EPOCH 16\n",
      "total_loss=0.08481: 100%|███████████████████████| 41/41 [00:14<00:00,  2.76it/s]\n",
      "INFO:PML:Evaluating epoch 16\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.03it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:New best accuracy! 0.5129096894798859\n",
      "INFO:PML:TRAINING EPOCH 17\n",
      "total_loss=0.08183: 100%|███████████████████████| 41/41 [00:15<00:00,  2.64it/s]\n",
      "INFO:PML:Evaluating epoch 17\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.04it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:New best accuracy! 0.514568440599043\n",
      "INFO:PML:TRAINING EPOCH 18\n",
      "total_loss=0.07442: 100%|███████████████████████| 41/41 [00:15<00:00,  2.71it/s]\n",
      "INFO:PML:Evaluating epoch 18\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 14.95it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:New best accuracy! 0.5162661410904782\n",
      "INFO:PML:TRAINING EPOCH 19\n",
      "total_loss=0.07172: 100%|███████████████████████| 41/41 [00:14<00:00,  2.77it/s]\n",
      "INFO:PML:Evaluating epoch 19\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.04it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:New best accuracy! 0.5243685345034375\n",
      "INFO:PML:TRAINING EPOCH 20\n",
      "total_loss=0.07323: 100%|███████████████████████| 41/41 [00:14<00:00,  2.79it/s]\n",
      "INFO:PML:Evaluating epoch 20\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.03it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:New best accuracy! 0.5256725481457825\n",
      "INFO:PML:TRAINING EPOCH 21\n",
      "total_loss=0.09849: 100%|███████████████████████| 41/41 [00:14<00:00,  2.76it/s]\n",
      "INFO:PML:Evaluating epoch 21\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.02it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:New best accuracy! 0.5279080647806426\n",
      "INFO:PML:TRAINING EPOCH 22\n",
      "total_loss=0.08016: 100%|███████████████████████| 41/41 [00:14<00:00,  2.79it/s]\n",
      "INFO:PML:Evaluating epoch 22\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.07it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:New best accuracy! 0.5355165693913697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:PML:TRAINING EPOCH 23\n",
      "total_loss=0.10987: 100%|███████████████████████| 41/41 [00:14<00:00,  2.77it/s]\n",
      "INFO:PML:Evaluating epoch 23\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 14.99it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:TRAINING EPOCH 24\n",
      "total_loss=0.08965: 100%|███████████████████████| 41/41 [00:14<00:00,  2.85it/s]\n",
      "INFO:PML:Evaluating epoch 24\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.11it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:New best accuracy! 0.540953991437213\n",
      "INFO:PML:TRAINING EPOCH 25\n",
      "total_loss=0.06549: 100%|███████████████████████| 41/41 [00:15<00:00,  2.73it/s]\n",
      "INFO:PML:Evaluating epoch 25\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.07it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:New best accuracy! 0.541999096513709\n",
      "INFO:PML:TRAINING EPOCH 26\n",
      "total_loss=0.07679: 100%|███████████████████████| 41/41 [00:15<00:00,  2.70it/s]\n",
      "INFO:PML:Evaluating epoch 26\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.03it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:TRAINING EPOCH 27\n",
      "total_loss=0.08013: 100%|███████████████████████| 41/41 [00:15<00:00,  2.63it/s]\n",
      "INFO:PML:Evaluating epoch 27\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.10it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:New best accuracy! 0.5482521994682431\n",
      "INFO:PML:TRAINING EPOCH 28\n",
      "total_loss=0.06741: 100%|███████████████████████| 41/41 [00:14<00:00,  2.74it/s]\n",
      "INFO:PML:Evaluating epoch 28\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.08it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:New best accuracy! 0.5490105453562086\n",
      "INFO:PML:TRAINING EPOCH 29\n",
      "total_loss=0.10526: 100%|███████████████████████| 41/41 [00:15<00:00,  2.65it/s]\n",
      "INFO:PML:Evaluating epoch 29\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.03it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:TRAINING EPOCH 30\n",
      "total_loss=0.08671: 100%|███████████████████████| 41/41 [00:15<00:00,  2.71it/s]\n",
      "INFO:PML:Evaluating epoch 30\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 14.64it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:TRAINING EPOCH 31\n",
      "total_loss=0.07900: 100%|███████████████████████| 41/41 [00:15<00:00,  2.68it/s]\n",
      "INFO:PML:Evaluating epoch 31\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.04it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:TRAINING EPOCH 32\n",
      "total_loss=0.09304: 100%|███████████████████████| 41/41 [00:14<00:00,  2.79it/s]\n",
      "INFO:PML:Evaluating epoch 32\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.06it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:New best accuracy! 0.5550485922448606\n",
      "INFO:PML:TRAINING EPOCH 33\n",
      "total_loss=0.08304: 100%|███████████████████████| 41/41 [00:15<00:00,  2.67it/s]\n",
      "INFO:PML:Evaluating epoch 33\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.05it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:TRAINING EPOCH 34\n",
      "total_loss=0.07283: 100%|███████████████████████| 41/41 [00:14<00:00,  2.80it/s]\n",
      "INFO:PML:Evaluating epoch 34\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.06it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:New best accuracy! 0.5629743358277965\n",
      "INFO:PML:TRAINING EPOCH 35\n",
      "total_loss=0.07369: 100%|███████████████████████| 41/41 [00:15<00:00,  2.71it/s]\n",
      "INFO:PML:Evaluating epoch 35\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.07it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:TRAINING EPOCH 36\n",
      "total_loss=0.07414: 100%|███████████████████████| 41/41 [00:14<00:00,  2.77it/s]\n",
      "INFO:PML:Evaluating epoch 36\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.04it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:TRAINING EPOCH 37\n",
      "total_loss=0.07354: 100%|███████████████████████| 41/41 [00:14<00:00,  2.78it/s]\n",
      "INFO:PML:Evaluating epoch 37\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 14.84it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:TRAINING EPOCH 38\n",
      "total_loss=0.07220: 100%|███████████████████████| 41/41 [00:14<00:00,  2.80it/s]\n",
      "INFO:PML:Evaluating epoch 38\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.05it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:TRAINING EPOCH 39\n",
      "total_loss=0.07987: 100%|███████████████████████| 41/41 [00:15<00:00,  2.71it/s]\n",
      "INFO:PML:Evaluating epoch 39\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.01it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:TRAINING EPOCH 40\n",
      "total_loss=0.06741: 100%|███████████████████████| 41/41 [00:14<00:00,  2.79it/s]\n",
      "INFO:PML:Evaluating epoch 40\n",
      "INFO:PML:Getting embeddings for the val split\n",
      "100%|███████████████████████████████████████████| 33/33 [00:02<00:00, 15.04it/s]\n",
      "INFO:PML:Computing accuracy for the val split w.r.t ['val']\n",
      "INFO:PML:Validation accuracy has plateaued. Exiting.\n"
     ]
    }
   ],
   "source": [
    "trainer.train(num_epochs=num_epochs)\n",
    "#выводимая метрика: mean average precision at r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Инференс и подсчет требуемой метрики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка модели (при необходимости)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunk = MetricLSTM()\n",
    "trunk.load_state_dict(torch.load('metric_lstm.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetricLSTM(\n",
       "  (word_embedding): Embedding(1193514, 25, padding_idx=0)\n",
       "  (word_lstm): LSTM(25, 25, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (char_embedding): Embedding(69, 25, padding_idx=0)\n",
       "  (char_lstm): LSTM(25, 25, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (linear): Linear(in_features=200, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunk.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Возвращаем валидационный датафрейм и считаем эмбеддинги на его объектах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>word_level</th>\n",
       "      <th>char_level</th>\n",
       "      <th>label_group</th>\n",
       "      <th>matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>[131370, 18071, 46352, 11005, 28, 32674, 37431...</td>\n",
       "      <td>[16, 16, 0, 39, 50, 46, 39, 42, 47, 0, 42, 54,...</td>\n",
       "      <td>6300</td>\n",
       "      <td>train_2406599165 train_3342059966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3342059966</td>\n",
       "      <td>[131370, 168647, 102671, 11235, 17, 792, 423, ...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>6300</td>\n",
       "      <td>train_2406599165 train_3342059966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_998568945</td>\n",
       "      <td>[8227, 6734, 289536, 40421, 3998, 5356, 76204,...</td>\n",
       "      <td>[21, 16, 21, 16, 0, 58, 21]</td>\n",
       "      <td>6475</td>\n",
       "      <td>train_998568945 train_3118756415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3118756415</td>\n",
       "      <td>[8227, 6734, 6734, 39601, 79764, 1494, 40421, ...</td>\n",
       "      <td>[17, 57, 51, 42, 0, 21, 16, 21, 16, 0, 42, 41,...</td>\n",
       "      <td>6475</td>\n",
       "      <td>train_998568945 train_3118756415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_1461445798</td>\n",
       "      <td>[18509, 10796, 11520, 35, 38, 490, 2274, 521, ...</td>\n",
       "      <td>[17, 0, 17, 0, 21, 0, 21]</td>\n",
       "      <td>6559</td>\n",
       "      <td>train_1461445798 train_1749322479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4198</th>\n",
       "      <td>train_3637239268</td>\n",
       "      <td>[202997, 150136]</td>\n",
       "      <td>[20, 16, 16, 45, 56, 0, 39, 52, 51, 59, 51]</td>\n",
       "      <td>5824</td>\n",
       "      <td>train_1643107217 train_3637239268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4199</th>\n",
       "      <td>train_3391985594</td>\n",
       "      <td>[449, 952, 37259, 39652, 39652, 46352, 11005, ...</td>\n",
       "      <td>[17, 18, 18, 21, 0, 17, 18, 18, 22, 0, 17, 18,...</td>\n",
       "      <td>5856</td>\n",
       "      <td>train_3391985594 train_1712739855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4200</th>\n",
       "      <td>train_1712739855</td>\n",
       "      <td>[37259, 39652, 28, 39652, 5555, 1568, 1617, 44...</td>\n",
       "      <td>[17, 18, 18, 21, 0, 49, 43, 51, 43, 48, 39, 50...</td>\n",
       "      <td>5856</td>\n",
       "      <td>train_3391985594 train_1712739855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4201</th>\n",
       "      <td>train_2944131255</td>\n",
       "      <td>[105361, 4088, 3039, 3190, 38, 38, 38, 72, 137...</td>\n",
       "      <td>[19, 16, 0, 39, 18, 16, 0, 39, 19, 16, 0, 39, ...</td>\n",
       "      <td>5552</td>\n",
       "      <td>train_2944131255 train_1001474240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4202</th>\n",
       "      <td>train_1001474240</td>\n",
       "      <td>[2630, 28, 105361, 4088, 369, 234, 986]</td>\n",
       "      <td>[39, 18, 16, 0, 39, 19, 16, 0, 39, 19, 16, 57,...</td>\n",
       "      <td>5552</td>\n",
       "      <td>train_2944131255 train_1001474240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4203 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            posting_id                                         word_level  \\\n",
       "0     train_2406599165  [131370, 18071, 46352, 11005, 28, 32674, 37431...   \n",
       "1     train_3342059966  [131370, 168647, 102671, 11235, 17, 792, 423, ...   \n",
       "2      train_998568945  [8227, 6734, 289536, 40421, 3998, 5356, 76204,...   \n",
       "3     train_3118756415  [8227, 6734, 6734, 39601, 79764, 1494, 40421, ...   \n",
       "4     train_1461445798  [18509, 10796, 11520, 35, 38, 490, 2274, 521, ...   \n",
       "...                ...                                                ...   \n",
       "4198  train_3637239268                                   [202997, 150136]   \n",
       "4199  train_3391985594  [449, 952, 37259, 39652, 39652, 46352, 11005, ...   \n",
       "4200  train_1712739855  [37259, 39652, 28, 39652, 5555, 1568, 1617, 44...   \n",
       "4201  train_2944131255  [105361, 4088, 3039, 3190, 38, 38, 38, 72, 137...   \n",
       "4202  train_1001474240            [2630, 28, 105361, 4088, 369, 234, 986]   \n",
       "\n",
       "                                             char_level  label_group  \\\n",
       "0     [16, 16, 0, 39, 50, 46, 39, 42, 47, 0, 42, 54,...         6300   \n",
       "1                                                   [0]         6300   \n",
       "2                           [21, 16, 21, 16, 0, 58, 21]         6475   \n",
       "3     [17, 57, 51, 42, 0, 21, 16, 21, 16, 0, 42, 41,...         6475   \n",
       "4                             [17, 0, 17, 0, 21, 0, 21]         6559   \n",
       "...                                                 ...          ...   \n",
       "4198        [20, 16, 16, 45, 56, 0, 39, 52, 51, 59, 51]         5824   \n",
       "4199  [17, 18, 18, 21, 0, 17, 18, 18, 22, 0, 17, 18,...         5856   \n",
       "4200  [17, 18, 18, 21, 0, 49, 43, 51, 43, 48, 39, 50...         5856   \n",
       "4201  [19, 16, 0, 39, 18, 16, 0, 39, 19, 16, 0, 39, ...         5552   \n",
       "4202  [39, 18, 16, 0, 39, 19, 16, 0, 39, 19, 16, 57,...         5552   \n",
       "\n",
       "                                matches  \n",
       "0     train_2406599165 train_3342059966  \n",
       "1     train_2406599165 train_3342059966  \n",
       "2      train_998568945 train_3118756415  \n",
       "3      train_998568945 train_3118756415  \n",
       "4     train_1461445798 train_1749322479  \n",
       "...                                 ...  \n",
       "4198  train_1643107217 train_3637239268  \n",
       "4199  train_3391985594 train_1712739855  \n",
       "4200  train_3391985594 train_1712739855  \n",
       "4201  train_2944131255 train_1001474240  \n",
       "4202  train_2944131255 train_1001474240  \n",
       "\n",
       "[4203 rows x 5 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#индексы сохранены предварительно на этапе создания датасетов\n",
    "val_df = df.iloc[val_idx, :].reset_index(drop=True)\n",
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_inference_tensor(array):\n",
    "    '''Служебная функция для перевода информации из датафрейма\n",
    "    в формат, требуемый на вход в MetricLSTM'''\n",
    "    tensor1 = torch.tensor(array[0]).unsqueeze(dim=0)\n",
    "    tensor2 = torch.tensor(array[1]).unsqueeze(dim=0)\n",
    "    inference_tensor = batch_pack(tensor1, tensor2)\n",
    "    return inference_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>word_level</th>\n",
       "      <th>char_level</th>\n",
       "      <th>label_group</th>\n",
       "      <th>matches</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>[131370, 18071, 46352, 11005, 28, 32674, 37431...</td>\n",
       "      <td>[16, 16, 0, 39, 50, 46, 39, 42, 47, 0, 42, 54,...</td>\n",
       "      <td>6300</td>\n",
       "      <td>train_2406599165 train_3342059966</td>\n",
       "      <td>[0.23456330597400665, -0.20327991247177124, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3342059966</td>\n",
       "      <td>[131370, 168647, 102671, 11235, 17, 792, 423, ...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>6300</td>\n",
       "      <td>train_2406599165 train_3342059966</td>\n",
       "      <td>[0.25036174058914185, -0.06551595777273178, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_998568945</td>\n",
       "      <td>[8227, 6734, 289536, 40421, 3998, 5356, 76204,...</td>\n",
       "      <td>[21, 16, 21, 16, 0, 58, 21]</td>\n",
       "      <td>6475</td>\n",
       "      <td>train_998568945 train_3118756415</td>\n",
       "      <td>[0.06010384485125542, 0.00598347932100296, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3118756415</td>\n",
       "      <td>[8227, 6734, 6734, 39601, 79764, 1494, 40421, ...</td>\n",
       "      <td>[17, 57, 51, 42, 0, 21, 16, 21, 16, 0, 42, 41,...</td>\n",
       "      <td>6475</td>\n",
       "      <td>train_998568945 train_3118756415</td>\n",
       "      <td>[-0.13363735377788544, 0.1606520414352417, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_1461445798</td>\n",
       "      <td>[18509, 10796, 11520, 35, 38, 490, 2274, 521, ...</td>\n",
       "      <td>[17, 0, 17, 0, 21, 0, 21]</td>\n",
       "      <td>6559</td>\n",
       "      <td>train_1461445798 train_1749322479</td>\n",
       "      <td>[0.10545273125171661, -0.0020801499485969543, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4198</th>\n",
       "      <td>train_3637239268</td>\n",
       "      <td>[202997, 150136]</td>\n",
       "      <td>[20, 16, 16, 45, 56, 0, 39, 52, 51, 59, 51]</td>\n",
       "      <td>5824</td>\n",
       "      <td>train_1643107217 train_3637239268</td>\n",
       "      <td>[0.19476942718029022, -0.07286263257265091, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4199</th>\n",
       "      <td>train_3391985594</td>\n",
       "      <td>[449, 952, 37259, 39652, 39652, 46352, 11005, ...</td>\n",
       "      <td>[17, 18, 18, 21, 0, 17, 18, 18, 22, 0, 17, 18,...</td>\n",
       "      <td>5856</td>\n",
       "      <td>train_3391985594 train_1712739855</td>\n",
       "      <td>[0.34747713804244995, -0.12604424357414246, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4200</th>\n",
       "      <td>train_1712739855</td>\n",
       "      <td>[37259, 39652, 28, 39652, 5555, 1568, 1617, 44...</td>\n",
       "      <td>[17, 18, 18, 21, 0, 49, 43, 51, 43, 48, 39, 50...</td>\n",
       "      <td>5856</td>\n",
       "      <td>train_3391985594 train_1712739855</td>\n",
       "      <td>[0.250297486782074, -0.11767282336950302, -0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4201</th>\n",
       "      <td>train_2944131255</td>\n",
       "      <td>[105361, 4088, 3039, 3190, 38, 38, 38, 72, 137...</td>\n",
       "      <td>[19, 16, 0, 39, 18, 16, 0, 39, 19, 16, 0, 39, ...</td>\n",
       "      <td>5552</td>\n",
       "      <td>train_2944131255 train_1001474240</td>\n",
       "      <td>[0.20644812285900116, -0.08114614337682724, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4202</th>\n",
       "      <td>train_1001474240</td>\n",
       "      <td>[2630, 28, 105361, 4088, 369, 234, 986]</td>\n",
       "      <td>[39, 18, 16, 0, 39, 19, 16, 0, 39, 19, 16, 57,...</td>\n",
       "      <td>5552</td>\n",
       "      <td>train_2944131255 train_1001474240</td>\n",
       "      <td>[0.19145651161670685, -0.04238223284482956, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4203 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            posting_id                                         word_level  \\\n",
       "0     train_2406599165  [131370, 18071, 46352, 11005, 28, 32674, 37431...   \n",
       "1     train_3342059966  [131370, 168647, 102671, 11235, 17, 792, 423, ...   \n",
       "2      train_998568945  [8227, 6734, 289536, 40421, 3998, 5356, 76204,...   \n",
       "3     train_3118756415  [8227, 6734, 6734, 39601, 79764, 1494, 40421, ...   \n",
       "4     train_1461445798  [18509, 10796, 11520, 35, 38, 490, 2274, 521, ...   \n",
       "...                ...                                                ...   \n",
       "4198  train_3637239268                                   [202997, 150136]   \n",
       "4199  train_3391985594  [449, 952, 37259, 39652, 39652, 46352, 11005, ...   \n",
       "4200  train_1712739855  [37259, 39652, 28, 39652, 5555, 1568, 1617, 44...   \n",
       "4201  train_2944131255  [105361, 4088, 3039, 3190, 38, 38, 38, 72, 137...   \n",
       "4202  train_1001474240            [2630, 28, 105361, 4088, 369, 234, 986]   \n",
       "\n",
       "                                             char_level  label_group  \\\n",
       "0     [16, 16, 0, 39, 50, 46, 39, 42, 47, 0, 42, 54,...         6300   \n",
       "1                                                   [0]         6300   \n",
       "2                           [21, 16, 21, 16, 0, 58, 21]         6475   \n",
       "3     [17, 57, 51, 42, 0, 21, 16, 21, 16, 0, 42, 41,...         6475   \n",
       "4                             [17, 0, 17, 0, 21, 0, 21]         6559   \n",
       "...                                                 ...          ...   \n",
       "4198        [20, 16, 16, 45, 56, 0, 39, 52, 51, 59, 51]         5824   \n",
       "4199  [17, 18, 18, 21, 0, 17, 18, 18, 22, 0, 17, 18,...         5856   \n",
       "4200  [17, 18, 18, 21, 0, 49, 43, 51, 43, 48, 39, 50...         5856   \n",
       "4201  [19, 16, 0, 39, 18, 16, 0, 39, 19, 16, 0, 39, ...         5552   \n",
       "4202  [39, 18, 16, 0, 39, 19, 16, 0, 39, 19, 16, 57,...         5552   \n",
       "\n",
       "                                matches  \\\n",
       "0     train_2406599165 train_3342059966   \n",
       "1     train_2406599165 train_3342059966   \n",
       "2      train_998568945 train_3118756415   \n",
       "3      train_998568945 train_3118756415   \n",
       "4     train_1461445798 train_1749322479   \n",
       "...                                 ...   \n",
       "4198  train_1643107217 train_3637239268   \n",
       "4199  train_3391985594 train_1712739855   \n",
       "4200  train_3391985594 train_1712739855   \n",
       "4201  train_2944131255 train_1001474240   \n",
       "4202  train_2944131255 train_1001474240   \n",
       "\n",
       "                                              embedding  \n",
       "0     [0.23456330597400665, -0.20327991247177124, -0...  \n",
       "1     [0.25036174058914185, -0.06551595777273178, 0....  \n",
       "2     [0.06010384485125542, 0.00598347932100296, -0....  \n",
       "3     [-0.13363735377788544, 0.1606520414352417, 0.0...  \n",
       "4     [0.10545273125171661, -0.0020801499485969543, ...  \n",
       "...                                                 ...  \n",
       "4198  [0.19476942718029022, -0.07286263257265091, -0...  \n",
       "4199  [0.34747713804244995, -0.12604424357414246, -0...  \n",
       "4200  [0.250297486782074, -0.11767282336950302, -0.1...  \n",
       "4201  [0.20644812285900116, -0.08114614337682724, 0....  \n",
       "4202  [0.19145651161670685, -0.04238223284482956, 0....  \n",
       "\n",
       "[4203 rows x 6 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df['embedding'] = val_df[['word_level', 'char_level']].values.tolist()\n",
    "val_df['embedding'] = val_df['embedding'].apply(to_inference_tensor)\n",
    "val_df['embedding'] = val_df['embedding'].apply(lambda x: \n",
    "                                                trunk(x).detach().numpy().tolist()[0])\n",
    "val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поиск объектов, близких по мере, и подсчет итоговой метрики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравнивая эмбеддинги по мере (в данном случае по косинусной близости) с определенным порогом, мы поочередно ищем объекты, с высокой вероятностью одинаковые с данным. Диапазон порога, который обычно используется, оценил бы как [0.7, 0.9]; остановился на значении 0.8. <br>\n",
    "<br>\n",
    "Необходимо сразу отметить, что реализованный в настоящий момент алгоритм вычисления схожих объектов квадратичный и нуждается в оптимизации. Ей будут способствовать решение локальной проблемы с faiss, а также адаптация под задачу соответствующих классов PML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_lists(y_true, y_pred):\n",
    "    '''slightly different input format relative to the original\n",
    "    input: two lists, output: float'''\n",
    "    y_true = [set(x.split()) for x in y_true]\n",
    "    y_pred = [set(x) for x in y_pred]\n",
    "    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)]) \n",
    "    len_y_true = np.array([len(x) for x in y_true])\n",
    "    len_y_pred = np.array([len(x) for x in y_pred])\n",
    "    f1 = 2 * intersection / (len_y_pred + len_y_true)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4202  /  4202\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from IPython.display import clear_output\n",
    "\n",
    "threshold = 0.80\n",
    "n = len(val_df)\n",
    "predicted_matches = [None]*len(val_df)\n",
    "\n",
    "for i in val_df.index[:n]:\n",
    "    clear_output(wait=True)\n",
    "    print(i, ' / ', n-1)\n",
    "    current_predicted_match = []\n",
    "    for j in val_df.index:\n",
    "        vec1 = np.array([val_df.iloc[i]['embedding']])\n",
    "        vec2 = np.array([val_df.iloc[j]['embedding']])\n",
    "        if cosine_similarity(vec1, vec2) >= threshold:\n",
    "            current_predicted_match.append(val_df.iloc[j]['posting_id'])\n",
    "    predicted_matches[i] = current_predicted_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Финальная оценка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалось добиться существенного улучшения на валидационной выборке по сравнению с бейзлайном (0.60 против 0.36)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.60\n"
     ]
    }
   ],
   "source": [
    "y_true = list(val_df['matches'][:n])\n",
    "y_pred = predicted_matches[:n]\n",
    "\n",
    "score = f1_score_lists(y_true, y_pred).mean()\n",
    "print('F1 score: ', f'{score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ссылки и справочные материалы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Материалы (из описания задачи):\n",
    "- https://github.com/scikit-learn-contrib/metric-learn\n",
    "- https://arxiv.org/pdf/1503.03832.pdf\n",
    "- https://github.com/KevinMusgrave/pytorch-metric-learning\n",
    "- https://arxiv.org/pdf/2003.08505.pdf\n",
    "\n",
    "Дополнительные материалы, которые оказались полезными: <br>\n",
    "- Ха Ву Тран (Microsoft), лекция по Metric Learning <br>\n",
    "https://www.youtube.com/watch?v=aU9yEwgrJ54 <br>\n",
    "- Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley. Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey <br>\n",
    "https://arxiv.org/abs/2201.09267"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
